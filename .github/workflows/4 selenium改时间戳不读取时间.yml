name: 4 Selenium IPv6 æ”¹ç”¨æ—¶é—´æˆ³

on:
  workflow_dispatch:
    inputs:
      max_ipv6_count:
        description: 'èšåˆæ–‡ä»¶ myIPv6.txt ä¸­ä¿ç•™çš„æœ€å¤§ IPv6 æ¡æ•°ã€‚'
        required: false
        default: '500' # é»˜è®¤å€¼ 500
        type: string
  workflow_run:
    workflows: ["2 Time"]
    types:
      - completed

concurrency:
  group: ${{ github.repository }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  fetch_and_sync_ipv6:
    runs-on: ubuntu-latest
    env:
      # âš ï¸ ç¡®ä¿åœ¨ TARGET_REPO ä»“åº“ä¸­æ­£ç¡®é…ç½®äº†ä»¥ä¸‹ Secrets:
      # IPV6_SOURCE_URLS: åŒ…å«è¦æŠ“å–çš„ IPv6 åˆ—è¡¨ URLï¼Œæ¯è¡Œä¸€ä¸ªã€‚
      # TARGET_REPO: ç›®æ ‡ä»“åº“çš„ owner/repo æ ¼å¼ï¼Œå¦‚ 'user/repo'ã€‚
      # TARGET_PAT: å…·æœ‰å¯¹ç›®æ ‡ä»“åº“å†™æƒé™çš„ Personal Access Tokenã€‚
      # TARGET_FOLDER: ç›®æ ‡ä»“åº“ä¸­å­˜æ”¾æ–‡ä»¶çš„è·¯å¾„ï¼Œå¦‚ 'ipv6_lists'ã€‚
      TZ: Asia/Shanghai
      # è®¾ç½®èšåˆæ–‡ä»¶ä¿ç•™çš„æœ€å¤§ IPv6 æ¡æ•° (ä» input æˆ–é»˜è®¤å€¼è·å–)
      MAX_IPV6_COUNT: ${{ github.event.inputs.max_ipv6_count || '500' }}

    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: âš™ï¸ Set Git Credentials
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"

      # --- å®‰è£… Python, Selenium å’Œ Chromeï¼ˆå¯èƒ½éœ€è¦ä¸€å®šæ—¶é—´ï¼‰ ---
      - name: ğŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: âš™ï¸ Install Chrome Browser and Dependencies
        run: |
          sudo apt-get update
          # å°è¯•å®‰è£… google-chrome-stableï¼ˆåœ¨æŸäº› runner ç¯å¢ƒå¯èƒ½å¤±è´¥ï¼‰
          sudo apt-get install -y google-chrome-stable || true
          # å¦‚æœ google-chrome-stable æœªå®‰è£…æˆåŠŸï¼Œå°½é‡å®‰è£… chromium-browser ä½œä¸ºå›é€€
          sudo apt-get install -y chromium-browser || true

      - name: âš™ï¸ Install Python Libraries (Selenium, WebDriver Manager & dateutil)
        run: |
          pip install --upgrade pip
          pip install selenium webdriver-manager python-dateutil

      # ---------------------------------------------
      - name: ğŸŒ Fetch IPv6 Addresses and Save (using Selenium)
        id: fetch
        env:
          URL_LIST: ${{ secrets.IPV6_SOURCE_URLS }}
          # æ³¨æ„ï¼šä¸è¦è¦†ç›– GITHUB_OUTPUTï¼ˆrunner ä¼šè‡ªåŠ¨æä¾›ï¼‰ï¼Œè¿™é‡Œåªä¼  URL_LIST
        run: |
          echo "Start fetching IPv6 sources using Selenium..."
          
          python3 - <<'PY'
          import os
          import sys
          from pathlib import Path
          from urllib.parse import urlparse
          
          # Selenium imports
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from selenium.webdriver.common.by import By
          from webdriver_manager.chrome import ChromeDriverManager
          
          # Config
          URL_LIST = os.environ.get('URL_LIST', '').strip()
          TARGET_DIR = Path('./ipv6_fetch_temp')
          COMMIT_DETAILS_BODY_FILE = Path("commit_details_body.txt")
          COMMIT_MESSAGE_FILE = Path("final_commit_message.txt")
          GITHUB_OUTPUT = os.environ.get('GITHUB_OUTPUT') # runner æä¾›çš„æ–‡ä»¶è·¯å¾„
          
          if not URL_LIST:
              print("WARNING: IPV6_SOURCE_URLS is empty. Skipping fetch operation.")
              if GITHUB_OUTPUT:
                  with open(GITHUB_OUTPUT, "a") as f:
                      f.write(f"files_created=0\n")
              COMMIT_MESSAGE_FILE.write_text("Skipped due to empty URL list.\n")
              sys.exit(0)
          
          TARGET_DIR.mkdir(parents=True, exist_ok=True)
          COMMIT_DETAILS_BODY_FILE.write_text("")
          
          urls = [u.strip() for u in URL_LIST.splitlines() if u.strip()]
          count = 0
          
          # Selenium headless setup
          driver = None
          try:
              chrome_options = Options()
              chrome_options.add_argument("--headless")
              chrome_options.add_argument("--no-sandbox")
              chrome_options.add_argument("--disable-dev-shm-usage")
              chrome_options.add_argument("--window-size=1920,1080")
              chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
              
              # å°è¯•ä½¿ç”¨ ChromeDriver Manager è‡ªåŠ¨å®‰è£…æˆ–æŸ¥æ‰¾é©±åŠ¨
              service = Service(ChromeDriverManager().install())
              driver = webdriver.Chrome(service=service, options=chrome_options)
              driver.set_page_load_timeout(30)
              print("Selenium WebDriver initialized successfully (Headless Chrome).")
          except Exception as e:
              # å¦‚æœ WebDriver åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ service (ä¾èµ–ç³»ç»Ÿå·²å®‰è£…çš„é©±åŠ¨æˆ–ç¯å¢ƒå˜é‡)
              try:
                  print(f"Initial WebDriver setup failed ({e}), trying fallback...")
                  driver = webdriver.Chrome(options=chrome_options)
                  driver.set_page_load_timeout(30)
                  print("Selenium WebDriver initialized successfully (Fallback).")
              except Exception as fallback_e:
                  print(f"ERROR: Failed to initialize Selenium WebDriver (Fallback failed): {fallback_e}", file=sys.stderr)
                  if GITHUB_OUTPUT:
                      with open(GITHUB_OUTPUT, "a") as f:
                          f.write(f"files_created=0\n")
                  COMMIT_MESSAGE_FILE.write_text("Failed to initialize WebDriver.\n")
                  sys.exit(1)
          
          # Fetch loop
          for url in urls:
              print(f"--- Processing URL: {url} ---")
              try:
                  hostname = urlparse(url).netloc.replace('.', '').replace(':', '')
                  if not hostname:
                      print(f"Skipping invalid URL: {url}")
                      continue
                  filename = TARGET_DIR / f"v6_source_{hostname}.txt"
                  
                  driver.get(url)
                  WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                  page_source = driver.page_source
                  
                  if page_source and len(page_source) > 100:
                      filename.write_text(page_source, encoding='utf-8')
                      lines = len(page_source.splitlines())
                      print(f"âœ… Fetched approx {lines} lines, saved as {filename}")
                      with COMMIT_DETAILS_BODY_FILE.open("a") as f:
                          f.write(f"- Fetched approx {lines} lines from {hostname}\n")
                      count += 1
                  else:
                      print(f"âš ï¸ Fetched content empty or too short from {url}. Skipping.")
                      if filename.exists():
                          os.remove(filename)
              except Exception as e:
                  print(f"âŒ Failed to fetch content from {url} (Selenium error: {e}). Skipping.")
          
          if driver:
              driver.quit()
          
          print(f"Finished fetching. Total files created: {count}")
          
          # Set output for subsequent steps via GITHUB_OUTPUT file (runner æä¾›)
          if GITHUB_OUTPUT:
              with open(GITHUB_OUTPUT, "a") as f:
                  f.write(f"files_created={count}\n")
          
          header_message = f"Auto update IPv6 sources ({count} files) via Selenium"
          body_content = COMMIT_DETAILS_BODY_FILE.read_text()
          COMMIT_MESSAGE_FILE.write_text(f"{header_message}\n\n{body_content}")
          PY

      - name: ğŸ’¾ Sync Files to Target Repository
        if: steps.fetch.outputs.files_created != '0'
        env:
          TARGET_URL: ${{ secrets.TARGET_REPO }}
          TARGET_PAT: ${{ secrets.TARGET_PAT }}
          TARGET_FOLDER: ${{ secrets.TARGET_FOLDER }}
          TEMP_DIR: ./ipv6_fetch_temp
          # MAX_IPV6_COUNT is inherited from job env
        run: |
          echo "Starting sync to target repository..."
          if [ -z "$TARGET_URL" ] || [ -z "$TARGET_PAT" ] || [ -z "$TARGET_FOLDER" ]; then
            echo "::error::TARGET_REPO / TARGET_PAT / TARGET_FOLDER not configured. Skipping."
            exit 0
          fi

          # è§„èŒƒåŒ–ç›®æ ‡ URLï¼ˆæ”¯æŒ owner/repo æˆ–å®Œæ•´ URLï¼‰
          TARGET_URL_RAW="${TARGET_URL}"
          if [[ $TARGET_URL_RAW == *"."* ]] || [[ $TARGET_URL_RAW == https://* ]]; then
            TARGET_URL_HOST=$(echo "$TARGET_URL_RAW" | sed 's#https://##' | sed 's#http://##')
          else
            TARGET_URL_HOST="github.com/${TARGET_URL_RAW}"
          fi

          TARGET_URL_WITH_PAT="https://${TARGET_PAT}@${TARGET_URL_HOST}"
          
          # å…‹éš†ç›®æ ‡ä»“åº“
          git clone --single-branch "$TARGET_URL_WITH_PAT" target_repo_sync
          cd target_repo_sync
          git checkout main || git checkout -b main

          mkdir -p "${TARGET_FOLDER}"
          git config core.fileMode false

          # å¤åˆ¶æ–°æŠ“å–çš„æ–‡ä»¶
          cp -f ../${TEMP_DIR}/*.txt "./${TARGET_FOLDER}/" || true

          # ç”Ÿæˆå¹¶æ›´æ–° myIPv6.txtï¼ˆèšåˆé€»è¾‘ - å·²ä¿®æ”¹ä¸ºå¹³å‡åˆ†é…å’Œåˆ†ç»„ï¼‰
          echo "Running IPv6 aggregation script (Fair Distribution)..."
          python3 - <<'PY'
          import os, sys, re, ipaddress, datetime
          from pathlib import Path
          from collections import defaultdict

          try:
              # å¼•å…¥ dateutil.tz ç”¨äºæ—¶åŒºå¤„ç†
              from dateutil import parser as dateparser, tz 
          except ImportError:
              print("ERROR: 'python-dateutil' library required but not found.", file=sys.stderr)
              sys.exit(1)

          # 1. è·å–é…ç½®å’Œå½“å‰æ—¶é—´ï¼ˆç»Ÿä¸€ä½¿ç”¨ä¸Šæµ·æ—¶é—´ UTC+8ï¼‰
          TARGET_FOLDER = os.environ.get('TARGET_FOLDER', '').strip()
          MAX_COUNT = int(os.environ.get('MAX_IPV6_COUNT', '500'))
          
          # å®šä¹‰ä¸Šæµ·æ—¶åŒº
          SHANGHAI_TZ = tz.gettz('Asia/Shanghai')

          # è·å–å½“å‰çš„ä¸Šæµ·æ—¶é—´ï¼ˆå¸¦æ—¶åŒºä¿¡æ¯ï¼‰
          now_shanghai_aware = datetime.datetime.now(SHANGHAI_TZ)
          # å°†å…¶è½¬æ¢ä¸ºæ— æ—¶åŒºä¿¡æ¯çš„ naive datetime å¯¹è±¡ï¼Œä½†å€¼ä»£è¡¨ä¸Šæµ·æ—¶é—´
          now = now_shanghai_aware.replace(tzinfo=None) 
          
          print(f"Current Run Time for new IPs (Shanghai Time, Naive): {now.strftime('%Y-%m-%d %H:%M:%S')}")

          if not TARGET_FOLDER:
              print("ERROR: TARGET_FOLDER not set")
              sys.exit(1)

          repo_root = Path.cwd()
          target_dir = repo_root / TARGET_FOLDER
          target_dir.mkdir(parents=True, exist_ok=True)

          myfile = target_dir / 'myIPv6.txt'
          # key=IP (canonical form), value=(datetime_obj (Shanghai naive), source)
          # ä½¿ç”¨ defaultdict ä»¥ä¾¿å¤„ç†å¤šä¸ªæ¥æºå¯èƒ½æŠ¥å‘ŠåŒä¸€IPï¼Œä½†æˆ‘ä»¬åªä¿ç•™æœ€æ—©å‘ç°çš„æ—¶é—´
          ip_map = {} 

          # helper: è§„èŒƒåŒ– datetime å¯¹è±¡ä¸ºä¸Šæµ·æ—¶é—´çš„ naive å¯¹è±¡
          def normalize_to_shanghai_naive(dt):
              if dt is None:
                  return None
              try:
                  # å¦‚æœæ˜¯å¸¦æ—¶åŒºçš„ aware å¯¹è±¡ï¼Œå…ˆè½¬æ¢åˆ°ä¸Šæµ·æ—¶åŒº
                  if dt.tzinfo is not None:
                      dt_shanghai = dt.astimezone(SHANGHAI_TZ)
                      # ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼Œä½¿å…¶æˆä¸º Shanghai naive
                      return dt_shanghai.replace(tzinfo=None)
                  
                  # å¦‚æœæ˜¯ naive å¯¹è±¡ï¼Œæˆ‘ä»¬å‡å®šå®ƒå·²ç»æ˜¯ä¸Šæµ·æ—¶é—´
                  return dt
              except Exception:
                  return None

          # 2. è¯»å–å·²å­˜åœ¨çš„ myIPv6.txtï¼ˆè§£ææ—§æ—¶é—´å¹¶è§„èŒƒåŒ–åˆ°ä¸Šæµ·æ—¶é—´ï¼‰
          if myfile.exists():
              print(f"Read existing file: {myfile}")
              try:
                  content = myfile.read_text(encoding='utf-8', errors='ignore')
                  lines = content.splitlines()
                  for line in lines:
                      if line.startswith('#') or line.startswith('-') or not line.strip():
                          continue
                      parts = line.split('|')
                      if len(parts) >= 2:
                          ip_str = parts[0].strip()
                          time_str = parts[1].strip()
                          # å…¼å®¹æ—§æ ¼å¼ï¼Œå¦‚æœæºä¸å­˜åœ¨ï¼Œåˆ™è®¾ä¸º Unknown
                          src_str = parts[2].strip() if len(parts) > 2 else "Unknown" 
                          try:
                              # è§£æ IP æ—¶ï¼Œå°†å…¶è½¬æ¢ä¸ºè§„èŒƒåŒ–å½¢å¼ä½œä¸ºå­—å…¸é”®
                              ip_obj = ipaddress.ip_address(ip_str)
                              canonical_ip = ip_obj.compressed 
                              
                              dt = dateparser.parse(time_str)
                              # è§„èŒƒåŒ–åˆ°ä¸Šæµ·æ—¶é—´ naive
                              dt = normalize_to_shanghai_naive(dt)
                              # æ£€æŸ¥æ—¶é—´åˆç†æ€§ (é˜²æ­¢è§£æé”™è¯¯å¯¼è‡´æ—¶é—´æˆ³å¼‚å¸¸)
                              if dt and 2000 <= dt.year <= 2035:
                                  # å¦‚æœ IP å·²ç»å­˜åœ¨ï¼Œä¿ç•™æ—§çš„æ—¶é—´æˆ³ï¼ˆæ›´æ—©å‘ç°çš„è®°å½•ï¼‰
                                  if canonical_ip not in ip_map or dt < ip_map[canonical_ip][0]:
                                      ip_map[canonical_ip] = (dt, src_str)
                          except Exception:
                              # å¿½ç•¥æ— æ³•è§£æçš„ IP æˆ–æ—¶é—´
                              pass
              except Exception as e:
                  print(f"Warning: Failed to parse existing file: {e}")

          print(f"Loaded {len(ip_map)} entries from existing file.")

          # 3. å¤„ç†æ–°æŠ“å–çš„ v6_source_*.txt æ–‡ä»¶ï¼ˆç»Ÿä¸€ä½¿ç”¨å½“å‰ä¸Šæµ·æ—¶é—´ä½œä¸ºæ–° IP çš„æ—¶é—´æˆ³ï¼‰
          files = sorted(target_dir.glob('v6_source_*.txt'))
          print(f"Processing {len(files)} new source files...")

          # ç»Ÿä¸€ä½¿ç”¨å½“å‰è¿è¡Œæ—¶é—´ï¼ˆä¸Šæµ·æ—¶é—´ naiveï¼‰ä½œä¸ºæ–° IP çš„æ—¶é—´æˆ³
          found_ts = now

          for f in files:
              # æ¥æºåç§°æ¥è‡ªæ–‡ä»¶å
              source_name = f.name.replace('v6_source_','').replace('.txt','')
              text = f.read_text(encoding='utf-8', errors='ignore')

              # æå–å¯èƒ½çš„ IPv6ï¼ˆç²—ç•¥è¿‡æ»¤åç”¨ ipaddress éªŒè¯ï¼‰
              tokens = set(re.findall(r'[0-9A-Fa-f:.]{2,}|[0-9A-Fa-f:]+', text))
              ips_canonical = set()
              for t in tokens:
                  if ':' not in t:
                      continue
                  t_clean = t.strip(" ,;\"'()[]<>")
                  try:
                      ip_obj = ipaddress.ip_address(t_clean)
                      if ip_obj.version == 6:
                          canonical_ip = ip_obj.compressed
                          ips_canonical.add(canonical_ip)
                  except Exception:
                      continue

              for ip in ips_canonical:
                  prev = ip_map.get(ip)
                  # å¦‚æœä¹‹å‰ä¸å­˜åœ¨ï¼Œåˆ™ä»¥ found_tsï¼ˆnowï¼‰åŠ å…¥
                  # å¦‚æœä¹‹å‰å­˜åœ¨ï¼Œä¸”æ–°æ—¶é—´æ›´æ—©ï¼ˆç†è®ºä¸Šä¸å¯èƒ½ï¼‰ï¼Œåˆ™æ›´æ–°ï¼›å¦åˆ™ä¿ç•™æ—§æ—¶é—´
                  if prev is None:
                      ip_map[ip] = (found_ts, source_name)
                  # âš ï¸ æ³¨æ„ï¼šå¦‚æœ IP å·²ç»å­˜åœ¨ï¼Œæˆ‘ä»¬ä¿ç•™å®ƒæœ€åˆè¢«è®°å½•çš„æ—¶é—´ï¼ˆå³ ip_map[ip][0]ï¼‰ã€‚
                  # å¦‚æœéœ€è¦æ›´æ–°ä¸ºæœ€æ–°çš„å‘ç°æ—¶é—´ï¼Œåˆ™æ”¹ä¸ºï¼šip_map[ip] = (max(prev[0], found_ts), source_name)
                  # è¿™é‡Œæˆ‘ä»¬ä¿æŒä¿å®ˆï¼Œä¿ç•™æœ€æ—©æ—¶é—´ï¼Œé¿å…é¢‘ç¹å˜åŠ¨ã€‚

          # 4. å‡†å¤‡åˆ†ç»„ã€è®¡ç®—é…é¢
          # è·å–æ‰€æœ‰IPçš„æ¥æºåˆ—è¡¨ (æŒ‰å­—æ¯é¡ºåºæ’åºï¼Œä»¥ä¾¿ç¡®å®šå¤„ç†é¡ºåº)
          unique_sources = sorted(list(set(v[1] for v in ip_map.values())))
          num_sources = len(unique_sources)
          
          if num_sources > 0:
              # å¹³å‡é…é¢ï¼ˆæ•´æ•°é™¤æ³•ï¼‰
              limit_per_source = MAX_COUNT // num_sources
              # å¤„ç†ä½™æ•°ï¼Œå°†å‰©ä½™é…é¢åˆ†é…ç»™å‰é¢çš„æº (ç®€å•å¤„ç†ï¼Œä¸å½±å“æ ¸å¿ƒé€»è¾‘)
              remainder = MAX_COUNT % num_sources
          else:
              limit_per_source = 0
              remainder = 0
              
          print(f"Total Unique IPs Found: {len(ip_map)}")
          print(f"Total Unique Sources: {num_sources}")
          print(f"Max Global Count: {MAX_COUNT}. Calculated Limit Per Source: {limit_per_source} (+{remainder} remainder)")

          kept_items_flat = [] # ç”¨äºæœ€ç»ˆå†™å…¥çš„ IPs (åŒ…å« ip, dt, src)
          
          if limit_per_source == 0 and remainder == 0:
              print("No sources or limit is zero. Generating empty file.")
          else:
              # 5. æŒ‰æ¥æºåˆ†ç»„
              # source_grouped_ips: {source_name: [(ip, dt, src), ...], ...}
              source_grouped_ips = defaultdict(list)
              for ip, (dt, src) in ip_map.items():
                  source_grouped_ips[src].append((ip, dt, src))

              # 6. ç»„å†…æ’åºå’Œæˆªæ–­
              total_kept = 0
              for i, src in enumerate(unique_sources):
                  ips_for_source = source_grouped_ips[src]
                  
                  # ç¡®å®šè¯¥æ¥æºçš„å®é™…é…é¢
                  current_limit = limit_per_source
                  if i < remainder: # åˆ†é…ä½™æ•°
                      current_limit += 1
                      
                  # ç»„å†…æ’åºï¼šæŒ‰æ—¶é—´æˆ³å€’åº (æœ€æ–°åœ¨å‰)
                  ips_for_source.sort(key=lambda x: x[1], reverse=True)
                  
                  # ç»„å†…æˆªæ–­
                  kept_in_group = ips_for_source[:current_limit]
                  kept_items_flat.extend(kept_in_group)
                  total_kept += len(kept_in_group)
                  
                  # å°†å¤„ç†åçš„åˆ—è¡¨å­˜å› grouped mapï¼Œä»¥ä¾¿ä¸‹ä¸€æ­¥çš„åˆ†ç»„è¾“å‡º
                  source_grouped_ips[src] = kept_in_group
                  
                  print(f"Source '{src}': Total found: {len(ips_for_source)}, Quota: {current_limit}, Kept: {len(kept_in_group)}")

              print(f"Total IPs kept after quotas: {total_kept}")

          # 7. ä¸ºæŠ¥å‘Šå¤´è®¡ç®—å…¨å±€ç»Ÿè®¡æ•°æ®
          time_range = "N/A"
          newest = None
          oldest = None
          if kept_items_flat:
              # è·å–æ‰€æœ‰ä¿ç•™ IP çš„æ—¶é—´ï¼Œç”¨äºç¡®å®šå…¨å±€æ—¶é—´èŒƒå›´
              times = [item[1] for item in kept_items_flat]
              newest = max(times)
              oldest = min(times)
              time_range = f"{oldest.strftime('%Y-%m-%d %H:%M:%S')} ~ {newest.strftime('%Y-%m-%d %H:%M:%S')}"
          
          generated_at = now.strftime('%Y-%m-%d %H:%M:%S') # ç”Ÿæˆæ—¶é—´æ˜¾ç¤ºä¸Šæµ·æ—¶é—´

          # 8. å†™å…¥æ–‡ä»¶ï¼šæŠ¥å‘Šå¤´
          header_lines = [
              "# IPv6 Summary Report (Grouped by Source, Fair Distribution Applied)",
              "# NOTE: All times are Shanghai Time (UTC+8)",
              f"Total Unique IPs Found (Merged Pool): {len(ip_map)}",
              f"Actual IPs Kept: {len(kept_items_flat)}",
              f"Applied Global Limit (MAX_IPV6_COUNT): {MAX_COUNT}",
              f"Calculated Limit Per Source: {limit_per_source} (+{remainder} remainder)",
              f"Time Range (Kept IPs): {time_range}",
              f"Generated At (Shanghai Time): {generated_at}",
              "-"*80,
          ]

          with myfile.open('w', encoding='utf-8') as fh:
              fh.write("\n".join(header_lines) + "\n")
              
              # 9. åˆ†ç»„è¾“å‡ºï¼šæŒ‰æ¥æºåç§°æ’åº (Alphabetical Order)
              for src in unique_sources:
                  kept_in_group = source_grouped_ips[src]
                  
                  if not kept_in_group:
                      continue
                      
                  # ç»„å¤´ï¼šæ˜¾ç¤ºæ¥æºåç§°ã€å®é™…ä¿ç•™æ•°é‡å’Œè¯¥æ¥æºçš„é…é¢
                  quota = limit_per_source + 1 if unique_sources.index(src) < remainder else limit_per_source
                  fh.write(f"\n# === Source: {src} ({len(kept_in_group)} / {quota} kept) ===\n")
                  fh.write("IPv6 | Time | Source\n")
                  fh.write("-" * 80 + "\n")

                  for ip, dt, _ in kept_in_group:
                      # ç»„å†…å·²æŒ‰æ—¶é—´å€’åºæ’åº (æœ€æ–°åœ¨å‰)
                      fh.write(f"{ip} | {dt.strftime('%Y-%m-%d %H:%M:%S')} | {src}\n")

          print(f"Done. Total merged: {len(ip_map)}. Total kept after quota: {len(kept_items_flat)}. Saved to {myfile}")
          PY

          # æäº¤å˜æ›´
          git add "${TARGET_FOLDER}" || true

          if git diff-index --quiet HEAD --; then
            echo "No changes detected in the target folder. Skipping commit/push."
          else
            COMMIT_FILE="${GITHUB_WORKSPACE}/final_commit_message.txt"
            CURRENT_BRANCH=$(git symbolic-ref --short HEAD)
            if git commit -F "$COMMIT_FILE"; then
              echo "Commit success. Pushing to origin/${CURRENT_BRANCH}..."
              git push origin "$CURRENT_BRANCH"
            else
              echo "::warning::Git commit failed but files was changed. Check logs."
            fi
          fi

      - name: âœ… Done
        run: echo "IPv6 fetch and aggregation complete."